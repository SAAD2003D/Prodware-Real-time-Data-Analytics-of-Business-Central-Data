version: '3.8'

services:
  spark: # This is your Spark Master service, named 'spark'
    image: docker.io/bitnami/spark:3.2.3
    container_name: spark_master_s3
    hostname: spark # For spark://spark:7077 resolution
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no
      - SPARK_RPC_ENCRYPTION_ENABLED=no
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
      # - BITNAMI_DEBUG=true # For verbose Bitnami startup logs
    ports:
      - '8080:8080' # Master Web UI
      - '7077:7077' # Master RPC Port
    volumes:
      # Path from this docker-compose.yml to your host's 'work' directory
      - ../../work:/opt/bitnami/spark/work # Example: if 'work' is at your_main_project_folder/work
    networks:
      - cdc_kafka_net
    healthcheck: {test: ["CMD", "nc", "-z", "localhost", "7077"], interval: 10s, timeout: 5s, retries: 5}


  spark-worker-1:
    image: docker.io/bitnami/spark:3.2.3
    container_name: spark_worker_1_s3
    hostname: spark-worker-1-host # Unique hostname
    depends_on:
      spark: # Depends on the service named 'spark' (your master)
        condition: service_healthy
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark:7077 # Points to the 'spark' service
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1 # Test with 1 core
      - SPARK_RPC_AUTHENTICATION_ENABLED=no # Match master
      - SPARK_RPC_ENCRYPTION_ENABLED=no     # Match master
    volumes:
      - ../../work:/opt/bitnami/spark/work # Match master
    networks:
      - cdc_kafka_net
  zookeeper:
    image: confluentinc/cp-zookeeper:7.4.0 # Using your specified version
    hostname: zookeeper
    container_name: zookeeper_cdc # Added _cdc suffix for clarity
    ports:
      - "2181:2181"
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    healthcheck:
      test: ['CMD', 'bash', '-c', "echo 'ruok' | nc localhost 2181 || exit 1"] # Added || exit 1 for better healthcheck
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cdc_kafka_net
   # volumes:
   #   - zookeeper_data_cdc:/data
   #   - zookeeper_datalog_cdc:/datalog
    

  kafka: # Renamed service from 'broker' to 'kafka' for clarity, matches BOOTSTRAP_SERVERS
    image: confluentinc/cp-server:7.4.0 # Using your specified version (cp-server includes Kafka broker)
    hostname: kafka_broker_host # Unique hostname inside container
    container_name: kafka_broker_cdc # Added _cdc suffix
    depends_on:
      zookeeper:
        condition: service_healthy
    ports:
      - "9092:9092" # For host access to Kafka
      # - "29092:29092" # Port for internal Docker network access, if different from host
      - "9101:9101" # JMX
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: 'zookeeper:2181' # Correct: zookeeper service name
      KAFKA_ZOOKEEPER_SESSION_TIMEOUT_MS: 36000

      # Listeners: How Kafka listens
      # INTERNAL: for communication within the Docker network (e.g., connect to kafka)
      # EXTERNAL: for communication from outside Docker (e.g., your laptop to kafka)
      KAFKA_LISTENERS: INTERNAL://0.0.0.0:29092,EXTERNAL://0.0.0.0:9092
      KAFKA_ADVERTISED_LISTENERS: INTERNAL://kafka_broker_host:29092,EXTERNAL://localhost:9092
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT
      KAFKA_INTER_BROKER_LISTENER_NAME: INTERNAL # For broker-to-broker communication if you had multiple brokers

      # KAFKA_CONFLUENT_SCHEMA_REGISTRY_URL: http://schema-registry:8081 # Commented out as schema-registry is not in this compose
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_GROUP_INITIAL_REBALANCE_DELAY_MS: 0
      KAFKA_CONFLUENT_LICENSE_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_CONFLUENT_BALANCER_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_TRANSACTION_STATE_LOG_MIN_ISR: 1
      KAFKA_TRANSACTION_STATE_LOG_REPLICATION_FACTOR: 1
      KAFKA_JMX_PORT: 9101
      KAFKA_JMX_HOSTNAME: kafka_broker_host # Use the internal hostname

      # Metrics Reporter (can keep if needed, but ensure it doesn't cause issues if not fully configured)
      KAFKA_METRIC_REPORTERS: io.confluent.metrics.reporter.ConfluentMetricsReporter
      CONFLUENT_METRICS_REPORTER_BOOTSTRAP_SERVERS: kafka_broker_host:29092 # Use internal listener
      CONFLUENT_METRICS_REPORTER_TOPIC_REPLICAS: 1
      CONFLUENT_METRICS_ENABLE: 'false' # Set to true if you want metrics
      CONFLUENT_SUPPORT_CUSTOMER_ID: 'anonymous'
    healthcheck:
      test: ['CMD', 'bash', '-c', "nc -z localhost 29092 || exit 1"] # Check internal listener; added || exit 1
      interval: 10s
      timeout: 5s
      retries: 5
    networks:
      - cdc_kafka_net
    volumes: 
      - kafka_data_cdc:/var/lib/kafka/data

  connect:
    image: debezium/connect:2.5 # Using your specified Debezium version
    hostname: connect_host # Unique hostname
    container_name: connect_cdc
    ports:
      - "8083:8083"
    depends_on:
      kafka: # Changed from depends_on: - kafka to depends_on: kafka: (service_healthy)
        condition: service_healthy # Wait for Kafka to be healthy
    environment:
      BOOTSTRAP_SERVERS: 'kafka_broker_host:29092' # Connect to Kafka's INTERNAL advertised listener
      GROUP_ID: '1'
      CONFIG_STORAGE_TOPIC: connect_configs_cdc
      CONFIG_STORAGE_REPLICATION_FACTOR: '1'
      OFFSET_STORAGE_TOPIC: connect_offsets_cdc
      OFFSET_STORAGE_REPLICATION_FACTOR: '1'
      STATUS_STORAGE_TOPIC: connect_statuses_cdc
      STATUS_STORAGE_REPLICATION_FACTOR: '1'
      CONNECT_KEY_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_KEY_CONVERTER_SCHEMAS_ENABLE: 'true'
      CONNECT_VALUE_CONVERTER: org.apache.kafka.connect.json.JsonConverter
      CONNECT_VALUE_CONVERTER_SCHEMAS_ENABLE: 'true'
    networks:
      - cdc_kafka_net
  kafdrop:
    image: obsidiandynamics/kafdrop
    container_name: kafdrop_cdc
    restart: always
    ports:
      - "9000:9000" 
    environment:
      KAFKA_BROKERCONNECT: "kafka_broker_host:29092" 
      JVM_OPTS: "-Xms32M -Xmx64M"
      SERVER_SERVLET_CONTEXTPATH: "/"
    networks:
      - cdc_kafka_net
    depends_on:
      - kafka

  spark-app-driver: # Your Spark application driver service
    build:
      # Path from this docker-compose.yml to the directory with your app's Dockerfile
      context: ../spark_streaming
      dockerfile: Dockerfile
    container_name: spark_app_driver_s3
    hostname: spark-app-driver-host # Hostname for spark.driver.host
    depends_on:
      - spark 
      - kafka # If your app connects to Kafka
        
    environment:
      SPARK_DRIVER_HOSTNAME: "spark-app-driver-host"
      KAFKA_BOOTSTRAP_SERVERS_CONFIG: "kafka_broker_host:29092" 
      KAFKA_TOPIC_CONFIG: "cdc.sqlserver.OUTIPRO.dbo.OUTIPRO_COPY_Location_437dbf0e-84ff-417a-965d-ed2bb9650972"
    networks:
      - cdc_kafka_net
    ports: # For Spark Driver UI from host
      - "4040:4040"
   
      


networks:
  cdc_kafka_net:
    driver: bridge

volumes: 
  kafka_data_cdc:
